We start with an idea of how to measure our language models, we do this with the per word cross entropy, which is basically the idea of how sure a model is of it’s a prediction. We start the idea of entropy, entropy is easiest to think about in terms of Huffman coding. You are trying to encode a message to someone and you are trying to compress a message. Imagine you have multiple letters that you are trying to encode, if they all appear with similar frequencies, then you can’t make it much more efficient. However if certain characters appear with more frequencies then you can do some optimizations and represent each character with a certain sequence of bits where the bits represent some characters, and you can assign the characters with higher frequencies a shorter number of bits and then you can assign the characters with less frequency a longer set of bits. Given a set of probabilities we compute the entropy by summing together the probabilities multiplied by the log of the probability. So to optimize our huffman coding, we want the bits expressed as close to the log of the probability as possible. The entropy is going to be less than the total bits required to express this text. We don't like this idea of whether we are sure of sentence or not, because as the sentence grows it is going to grow too, this doesn't work for us because our model is going to be the same throughout the sentence and it isn't going to become worse as it grows. So we divide it by the number of words to get our per word entropy. Then this leads us to have to talk about the per word entropy which is just the entropy divided by the number of words in the sentence. After establishing entropy we can talk about the cross entropy which is going to be our unit of measurment for our various models. The cross entropy in our case is going to be computed the same way that we computed the entropy, and we will use the per-word cross entropy for similar reasoning as to why you would use per word entropy.
Let's start by introducing an Hidden Markov Model, the hidden Markov Model is represented with 3 objects 1. a list a states, 2. a list of the outputs, and then a list of the transitions between the states and outputs. We predict the likelyhood of generating a sentence with the following formula we sum up all the probabilties of generating that sentence with that ends with the state. We are going to get this clear right now, when we end up with a state we always only care about the most likely way that we are going to end up in that state. After we know we want to generate a sentence that ends with a certain state, we go through each of the states and then we find the probability of transitioning from the current state to the next state with the output label, we use the multiply symbol to multiply all these probabilities better. HMM have multiple applications one of the best one is to use hMM as part of speech taggings. We first note that if we go through all the possibilites for the parts of speech for the sentence and sum them all up we are going to get the number one. Then we start with finding the equation that probability of the tags being in a certain order given that all the words is the probability of the words in that order and the tags in that order divided by the space which is going to be the probability of the words being that order in this case. We use the states similar to the way that we use tags and this allows us to use the standard HMM algorithim to identify parts of words. We also use a similar idea to try and recongnize text from speech. Of course we won't be going over these applications right now, but just be aware that HMM's can be used to do amazing things.
Now let's get to the meaty things, we are going to go through the algorithims that make the HMM's work. Let's first find the most likely path of a object and how to use an HMM to generate the most likely path. We first start with an empty state and an empty string. Then we go through each possible state and note the sequence in which we got there, and then the state we ended up in. We go through all of the states we end up in and we take the one with the maximum probability, we do this because once we are in a state, we do not care about how we got there and we are always going to want the most likely way to get to this state. We only care about the maximum proability because at the end, we are trying to find the maximum way we can go to get the desired sequence of words that we want. We can throw away all of the ways to get to this state that aren't the maximum probability. We use a special symbol if we weant to denote that we want a certain state sequence instead of only caring about the string sequence. After we do this we can do some more things involving HMM's which include calculating the proability and eventually training this model.
There are generally two parts to probability that you should remember, the forward probability and then backward probability. The forward probability is defined at time step t as the probability of getting a paticular sequence of words, namely the word sequence between 1 and t-1 and then ending up with state si which we can consider as the end of line in this small world that we consider with the forward probability. We can observe that the forward probaility when the time frame is zero is going to be 100% because when you start with an empty string, you are already going to be at an empty string. Another observation is that you can sum up the forward probabilities to a point, and if you just vary the states then it is going to be the probability of generating that particular sentence. We can also note that because we have a base case, we can use recurision to express the forward probabilite its. We are going to go through it like this we can sum up the forward probabilities to a point right before it. Since we know the state that we want to go to, and the current state of the forward probability, we can multiply them together and then sum up all the forward proabailities to get the forward probability of the next time step. Another useful concept is backwards probability, as you can probably tell it is very similar to forward probablility but instead of being forward it is... you probably gueslesed it. Backwards. The backwards probability of let's say time step t is going to be the probability of words t through n along with the probability of the state i at time step t. We use algorithims very similar to the forward probability to find the backwards proability. Now you may be asking what is the point of having a backwards probability? Well, in most contexts it is not particularly useful, but when you are training the model, the best thing to use is the backwards probabiltiy. WE note that the backwards probability of time step 1 is going to be the probability of generating the entire word sequence. This happens because the state is automatically going to be state 1 so it is just going to be the probability of generating the entire sentence.
Now let's get to training the model, remember that the main thing that we are going to be training is going to be the transitions between the states. So our objective is to find a function C that finds the number of times a transition happens. We want to set that transition probability to the amount of times that the transition happens over all of the possible transitions. Now the key problem that we must focus on is how we count the number of times that a transition is used. Here's what we do, we go through everything and see the probability of getting to that place is. We give the transitions a count, and then look at how many times that particular transition happens. After we get this count we use this count and get the probability of this transition by going through each of the possible transitions and then multiplying it by the probability of the path and then we add it up to see what is likely. We have to remember that because path is based on transitions that when we train the transition probabilities we are effectively training the entire model.
We start the idea of a PCFG with this idea of domination a non terminal dominates two other non terminals if that thing can be derived from it. For example if we say that a vp is derived from a verb an a noun phrase so therefore the verb phrase dominates the two non terminals. To get the probability of a certain sentence we do it as follows we first use the conventional tree building diagram and then we find the probability of each transition and then we multiply all the transitions together to get the total probabilioty to get the probability of the total sentence. There are also things like forward probability and backwards probability in PCFG's, for forward probabilities it is the probability of generating a part of the sentence within that part. It is also known as the outside probability, and conversely there is a thing called inside probability and this is the things outside a certain set of words. To find the backwards probability we go through all of the m's as a middle index. Then we every iterate through every possible p and q which in this case are the non terminal symbols that represent what happens to our word phrases. After this you should sum up all of these probabilities to get the inside probability. After we go through this we realize that we can do recursion on this problem so we modify the equation so that it only involves the changing of non terminals and the inside probabilities of the midpoint. After this we want to know how to compute the outside probabilties. In this case we want to know what is the probability of the outside words lining up the way that they lined up and the current nonterminal to be the nonterminal that it is. Since we are trying to find the probability of the thing from the outside we know that the two nonterminals beneath it could be switched around so we account for both possibilities, we need to seperate them because we have two places where we are accounting for it, so we split the midpoint before and then we split the midpoint after. Eventually you realize that you can do some recursion on this stuff so you do that. Training PCFG's are very similar to training HMM's. We find the C function by doing things as followed we try to find the probability that given the words that we go through each word substring that we get it
